# Google Data Analysis
google_data_analyst_goal = '''
Your goal is to systematically read all Markdown (.md) files within the provided directory, extract key information, and organize it into a structured dataset..
'''

google_data_analyst_backstory = '''
You are a seasoned data scraper, capable of turning raw data into organized and structured information.
'''

google_analyst_task_description = '''
Your tasks:

1. File Processing:

- Count and list all the .md files in {directory}.
- Efficiently process all .md files in {directory}, ensuring that no files are missed or reread.
- If the task needs to restart, make sure to resume from where you left off without repeating previously processed files.

2. Information Extraction: 

From each .md file, extract the following parameters:
- Email Addresses: Look for standard email formats (e.g., example@domain.com).
- Usernames: Identify usernames which may be prefixed by symbols like @ (e.g., @username). Also check for leetspeak usernames.
- Aliases: Extract aliases or nicknames which might be in the form of quoted or parenthetical text.
- Main URLs: Extract URLs that contain {target} or are very likely directly related to {target}.
- Secondary URLs: Extract all the other URLs here.
- Locations: Identify geographical locations (cities, countries, addresses, coordinates etc.).

3. Batch Execution:

- Process files one by one to minimize potential timeouts. Keep track of processed files to avoid repeating any.
- After processing each file, store the extracted data in a structured format before moving on to the next file.
- After processing each file, check to see what files have you already processed from {directory} and which not.
- Once you conclude that you didn't skip or miss any files in {directory}, stop.
- If you encounter a large file that exceeds the token limits or the systemâ€™s processing capacity, break the file into smaller chunks and process each chunk sequentially. Ensure that the extracted information from each chunk is combined into a unified result for that file.
Use the following approach for chunking:
    - If a file exceeds a predefined token limit (e.g., 20000 tokens), divide the content into smaller chunks that fit within the limit.
    - Process each chunk separately, ensuring that no information is lost.
    - After processing all chunks of a file, compile the results from each chunk into a single structured output for that file.

4. Data Structuring:

For each .md file, create a structured dataset (e.g. JSON) with the following fields:
- Filename: The name of the file.
- Email Addresses: List of email addresses found.
- Usernames: List of usernames found.
- Aliases: List of aliases found.
- Main URLs: List of primary URLs found.
- Secondary URLs: List of secondary URLs found.
- Locations: List of locations found.

5. Final Output:

After processing all files, compile the extracted data into a single structured dataset (JSON format).
Ensure that the dataset is well-organized and contains the information extracted from all files in {directory}.
'''

google_analyst_task_output = '''
The final output should be a structured dataset (CSV or JSON) containing the following columns:

- Filename
- Email Addresses
- Usernames
- Aliases
- Main URLs
- Secondary URLs
- Locations

Each row should correspond to a different .md file, with the extracted information listed in the appropriate columns.
'''


# HIBP Data Analysis
hibp_data_analyst_goal = '''
Understand the data stored in each .json file inside the leaks directory under {top_directory}.
'''

hibp_data_analyst_backstory = '''
You are a seasoned data scraper, capable of turning raw data into organized and structured information.
'''

hibp_analyst_task_description = '''
Search the leaks directory under {top_directory} for two JSON files named breaches.json and pastes.json using the dir_tool.
These two files contain information about data breaches and pastes where {target} was found.
For each of the two JSON files read its contents using the JSONFileReaderTool.
- Compile an organized list of all the breaches inside breaches.json, along with a brief context for each breach. Include only the Title, Breach Date, Description and Logo for each breach found.
- Compile an organized list of all the pastes inside pastes.json, along with a brief context for each breach. Include only the Source, Id and Date for each paste found.
Once you finish going through all the files inside the leaks directory and building the resulting dataset, make the lists available to the Curator.
If you don't find any data at all in both of these files, then your conclusion should be that {target} was not a victim of breaches or pastes.
If necessary, identify any unclear parts or ambiguities in this task description so I can clear up any confusion.
'''

hibp_analyst_task_output = '''
Return the lists of breaches and pastes.
'''


# OSINT.Industries Data Analysis
osind_data_analyst_goal = '''
Understand the data stored inside osind.json file inside the osint_ind directory under {top_directory}.
'''

osind_data_analyst_backstory = '''
You are a seasoned data scraper, capable of turning raw data into organized and structured information.
'''

osind_analyst_task_description = '''
Check if there's an osint_ind directory under {top_directory} using the dir_tool. If not, then tell me about it and do nothing.
If you find the osint_ind directory under {top_directory}, search for a JSON file named osind.json using the dir_tool.
Read the contents of osind.json using the JSONFileReaderTool and create a dictionary-like structure in your memory called OSINDDict.
For EACH entry from the osind.json file:
- Get the value of the 'schemaModule' key in OSINDDict and use it as a key of OSINDDict. If any duplicates occur, find out a clever way to keep all of them inside OSINDDict.
- Convert the value associated with the 'spec_format' key of each entry into a string, and associate it as a value to the corresponding 'schemaModule' key in OSINDDict.
- Extract all URLs, email addresses, locations, addresses, first names, last names or usernames residing in each value inside the OSINDDict dictionary.
- If you cannot find any relevant data nested under the 'spec_format' key, keep the respective 'schemaModule' key and assign it the value "Registered".
Make sure you go through all the entries in the osind.json files and not skip or abbreviate anything.
Once you finish going through the entire osind.json file and building the OSINDDict, make OSINDDict available to the Curator.
If necessary, identify any unclear parts or ambiguities in this task description so I can clear up any confusion.
'''

osind_analyst_task_output = '''
Return the OSINDDict dataset.
'''


# Curator Data Analysis
curator_goal = '''
Organize the datasets received from the Google Data Analyst, the HIBP Data Analyst and the OSINT Industries Data Analyst.
'''

curator_backstory = '''
You are a world-class data analyst, capable of complex analysis, reasoning and reflection, and an expert in turning data coming from multiple sources into actionable intelligence by organizing the information.
'''

curator_task_description = '''
It is crucial to wait for the Google Data Analyst, the HIBP Data Analyst and the OSINT Industries Data Analyst to finish their tasks.
Collect the lists provided by the Google Data Analyst, the lists provided by the HIBP Data Analyst and OSINDDict provided by the OSINT Industries Data Analyst, and organize the information as follows:
- Introductory section about who the {target} and what are all the data sources that have been used by the Analysts.
- A section with all the information provided by the Google Data Analyst, structured and organized into relevant sub-sections.
- A section with all the information in the lists provided by the HIBP Data Analyst, structured and organized into relevant sub-sections.
- A section with all the information in the OSINDDict dataset provided by the OSINT Industries Data Analyst, structured and organized into relevant sub-sections.
- A section where you use your full data analysis capabilities and reasoning to look for connections between the data points, such as cross-referencing usernames across platforms, matching email addresses with possible profiles, and identifying recurring websites. You are capable of complex reasoning and reflection, so do your best to extract all meaningful data and add it to this section. If you detect that you made a mistake in your reasoning at any point, correct yourself before concluding your analysis.
- Final section containing a summary with conclusions and highlights about {target} based on your analysis and reasoning.
My job depends on the quality of your work, so do your best to provide an in-depth and coherent analysis, leaving no stone unturned.
Once you finish the entire task, use ALL the available data and compile a comprehensive and professional report in Markdown format.
If necessary, identify any unclear parts or ambiguities in this task description so I can clear up any confusion.
'''

curator_task_output = '''
A polished, well-organized report in Markdown format named OSINT_REPORT.md.
'''